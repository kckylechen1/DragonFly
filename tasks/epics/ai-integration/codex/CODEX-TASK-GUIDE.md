# ğŸŸ¢ Codex ä»»åŠ¡æŒ‡å—: æµå¼å“åº” + å¤šæ¨¡å‹è·¯ç”±

> **è´Ÿè´£ Agent**: Codex (GPT-5.2)  
> **é¢„è®¡æ—¶é—´**: 4-5 å°æ—¶  
> **å¹¶è¡Œç»„**: B (Phase 2-3 åœ¨ GLM Phase 1 ä¹‹å)

---

## âš ï¸ é‡è¦æé†’

```
æŒ‰ AI-COLLAB-PLAYBOOK å·¥ä½œ
é‡åˆ°é—®é¢˜ç«‹å³åœä¸‹ï¼Œä¸è¦çŒœæµ‹
æ¯å®Œæˆä¸€ä¸ªä»»åŠ¡è¿è¡Œ pnpm check éªŒè¯
```

---

## âš ï¸ æ–‡ä»¶æ‰€æœ‰æƒå£°æ˜

### âœ… æœ¬ä»»åŠ¡æ‹¥æœ‰ (å¯ä¿®æ”¹)
- `client/src/refactor_v2/api/useAIStream.ts` (æ–°å»º)
- `client/src/refactor_v2/api/index.ts` (æ·»åŠ å¯¼å‡º)
- `server/routers/ai.ts`
- `server/_core/model-router.ts` (æ–°å»º)
- `server/_core/llm.ts`

### ğŸ”’ åªè¯»å‚è€ƒ (ä¸è¦ä¿®æ”¹)
- `server/_core/agent/smart-agent.ts`
- `server/_core/agent/orchestrator.ts` (GLM è´Ÿè´£)
- `server/_core/agent/types.ts`

### ğŸš« ç¦æ­¢è§¦ç¢° (GLM è´Ÿè´£)
- `client/src/refactor_v2/components/FloatingAIChatInput.tsx`
- `client/src/refactor_v2/components/AIChatPanel.tsx`
- `client/src/refactor_v2/stores/aiChat.store.ts`

---

## ğŸ“‹ ä»»åŠ¡æ¸…å•

### CDX-001: useAIStream Hook å®ç° [Phase 2]

**ç›®æ ‡**: åˆ›å»ºåŸºäº SSE çš„æµå¼å“åº” hook

**Step 1: åˆ›å»ºæ–°æ–‡ä»¶**

åˆ›å»º `client/src/refactor_v2/api/useAIStream.ts`:

```typescript
import { useState, useCallback, useRef } from "react";

export interface StreamProgress {
  type: "thinking" | "tool_call" | "tool_result" | "content" | "done" | "error";
  data: any;
}

export interface TodoStep {
  id: string;
  title: string;
  status: "pending" | "in_progress" | "completed" | "failed" | "skipped";
  toolName?: string;
  resultPreview?: string;
}

export function useAIStream() {
  const [isStreaming, setIsStreaming] = useState(false);
  const [streamContent, setStreamContent] = useState("");
  const [progress, setProgress] = useState<TodoStep[]>([]);
  const [error, setError] = useState<string | null>(null);
  const eventSourceRef = useRef<EventSource | null>(null);

  const startStream = useCallback(
    async (message: string, options?: { stockCode?: string; sessionId?: string }) => {
      // æ¸…ç†ä¹‹å‰çš„è¿æ¥
      if (eventSourceRef.current) {
        eventSourceRef.current.close();
      }

      setIsStreaming(true);
      setStreamContent("");
      setProgress([]);
      setError(null);

      try {
        // æ„å»º SSE URL
        const params = new URLSearchParams({
          message,
          ...(options?.stockCode && { stockCode: options.stockCode }),
          ...(options?.sessionId && { sessionId: options.sessionId }),
        });

        const url = `/api/ai/stream?${params.toString()}`;
        const eventSource = new EventSource(url);
        eventSourceRef.current = eventSource;

        eventSource.onmessage = (event) => {
          try {
            const data: StreamProgress = JSON.parse(event.data);

            switch (data.type) {
              case "thinking":
                // æ›´æ–°æ€è€ƒçŠ¶æ€
                break;

              case "tool_call":
                // æ·»åŠ æ–°çš„å·¥å…·è°ƒç”¨æ­¥éª¤
                setProgress((prev) => [
                  ...prev,
                  {
                    id: data.data.toolCallId || `step_${Date.now()}`,
                    title: `è°ƒç”¨ ${data.data.name}`,
                    status: "in_progress",
                    toolName: data.data.name,
                  },
                ]);
                break;

              case "tool_result":
                // æ›´æ–°å·¥å…·ç»“æœ
                setProgress((prev) =>
                  prev.map((step) =>
                    step.toolName === data.data.name
                      ? {
                          ...step,
                          status: data.data.ok ? "completed" : "failed",
                          resultPreview: data.data.result?.slice(0, 100),
                        }
                      : step
                  )
                );
                break;

              case "content":
                // è®¾ç½®æœ€ç»ˆå†…å®¹
                setStreamContent(data.data);
                break;

              case "done":
                eventSource.close();
                setIsStreaming(false);
                break;

              case "error":
                setError(data.data);
                eventSource.close();
                setIsStreaming(false);
                break;
            }
          } catch (parseError) {
            console.error("Failed to parse SSE event:", parseError);
          }
        };

        eventSource.onerror = (err) => {
          console.error("SSE error:", err);
          setError("è¿æ¥ä¸­æ–­ï¼Œè¯·é‡è¯•");
          eventSource.close();
          setIsStreaming(false);
        };
      } catch (err) {
        setError(err instanceof Error ? err.message : "æœªçŸ¥é”™è¯¯");
        setIsStreaming(false);
      }
    },
    []
  );

  const stopStream = useCallback(() => {
    if (eventSourceRef.current) {
      eventSourceRef.current.close();
      eventSourceRef.current = null;
    }
    setIsStreaming(false);
  }, []);

  return {
    isStreaming,
    streamContent,
    progress,
    error,
    startStream,
    stopStream,
  };
}
```

**Step 2: æ›´æ–°å¯¼å‡º**

ä¿®æ”¹ `client/src/refactor_v2/api/index.ts`:

```typescript
export { api } from "./client";
export * from "./stocks";
export * from "./watchlist";
export * from "./ai";
export * from "./useAIStream"; // æ–°å¢
```

**Step 3: éªŒè¯**
```bash
pnpm check
```

---

### CDX-002: åç«¯ SSE æµå¼ç«¯ç‚¹ [Phase 2]

**ç›®æ ‡**: æ·»åŠ  SSE ç«¯ç‚¹è¿æ¥ SmartAgent.stream()

**Step 1: ä¿®æ”¹ server/routers/ai.ts**

åœ¨ç°æœ‰è·¯ç”±åŸºç¡€ä¸Šæ·»åŠ æµå¼ç«¯ç‚¹:

```typescript
import { createSmartAgent } from "../_core/agent";

// åœ¨ aiRouter ä¸­æ·»åŠ 
streamChat: publicProcedure
  .input(
    z.object({
      message: z.string(),
      sessionId: z.string().optional(),
      stockCode: z.string().optional(),
      useThinking: z.boolean().optional(),
    })
  )
  .mutation(async function* ({ input }) {
    const agent = createSmartAgent({
      sessionId: input.sessionId,
      stockCode: input.stockCode,
      thinkHard: input.useThinking,
    });

    for await (const event of agent.stream(input.message)) {
      yield event;
    }
  }),
```

**æ³¨æ„**: tRPC å¯èƒ½ä¸ç›´æ¥æ”¯æŒ AsyncGeneratorã€‚å¦‚æœé‡åˆ°é—®é¢˜ï¼Œä½¿ç”¨åŸç”Ÿ Express è·¯ç”±:

**å¤‡é€‰æ–¹æ¡ˆ: åŸç”Ÿ Express SSE**

åœ¨ `server/_core/index.ts` æˆ–åˆ›å»ºæ–°æ–‡ä»¶ `server/_core/sse-handler.ts`:

```typescript
import { createSmartAgent } from "./agent";
import type { Request, Response } from "express";

export async function handleAIStream(req: Request, res: Response) {
  const { message, stockCode, sessionId } = req.query as {
    message: string;
    stockCode?: string;
    sessionId?: string;
  };

  if (!message) {
    res.status(400).json({ error: "message is required" });
    return;
  }

  // è®¾ç½® SSE å¤´
  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.setHeader("Connection", "keep-alive");
  res.flushHeaders();

  const agent = createSmartAgent({
    sessionId,
    stockCode,
  });

  try {
    for await (const event of agent.stream(message)) {
      res.write(`data: ${JSON.stringify(event)}\n\n`);
    }
    res.write(`data: ${JSON.stringify({ type: "done" })}\n\n`);
  } catch (error) {
    res.write(
      `data: ${JSON.stringify({ type: "error", data: (error as Error).message })}\n\n`
    );
  } finally {
    res.end();
  }
}
```

ç„¶ååœ¨ Express åº”ç”¨ä¸­æ³¨å†Œ:
```typescript
app.get("/api/ai/stream", handleAIStream);
```

**Step 2: éªŒè¯**
```bash
pnpm check
```

---

### CDX-003: å¤šæ¨¡å‹è·¯ç”±ç³»ç»Ÿ [Phase 3]

**ç›®æ ‡**: åˆ›å»ºæ”¯æŒ GLM/Grok/Qwen/DeepSeek çš„æ¨¡å‹è·¯ç”±

**Step 1: åˆ›å»º model-router.ts**

åˆ›å»º `server/_core/model-router.ts`:

```typescript
/**
 * å¤šæ¨¡å‹è·¯ç”±ç³»ç»Ÿ
 *
 * æ”¯æŒçš„æ¨¡å‹:
 * - GLM-4.7 (æ™ºè°±) - ä¸­æ–‡èƒ½åŠ›å¼ºï¼Œæˆæœ¬ä½
 * - Grok-4.1 (xAI) - å®æ—¶æœç´¢èƒ½åŠ›
 * - Qwen (ç¡…åŸºæµåŠ¨) - é€šç”¨èƒ½åŠ›ï¼Œæˆæœ¬ä½
 * - DeepSeek (ç¡…åŸºæµåŠ¨) - æ¨ç†èƒ½åŠ›å¼º
 */

import { getEnvConfig } from "./env";

export type ModelProvider = "glm" | "grok" | "qwen" | "deepseek";

export interface ModelConfig {
  name: string;
  provider: ModelProvider;
  apiKey: string;
  endpoint: string;
  model: string;
  capabilities: string[];
  costTier: 1 | 2 | 3; // 1 = ä¾¿å®œ, 3 = è´µ
  speedTier: 1 | 2 | 3; // 1 = å¿«, 3 = æ…¢
  maxTokens: number;
}

export interface ModelPreference {
  provider?: ModelProvider;
  reason?: string;
  capabilities?: string[];
}

export interface ChatMessage {
  role: "system" | "user" | "assistant";
  content: string;
}

export interface LLMResponse {
  content: string | null;
  model: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

// æ¨¡å‹æ³¨å†Œè¡¨
function getModelRegistry(): ModelConfig[] {
  const env = getEnvConfig();

  return [
    {
      name: "GLM-4.7",
      provider: "glm",
      apiKey: env.GLM_API_KEY || "",
      endpoint: "https://open.bigmodel.cn/api/paas/v4/chat/completions",
      model: "glm-4-plus",
      capabilities: ["chinese", "code", "fast", "cheap", "general"],
      costTier: 1,
      speedTier: 1,
      maxTokens: 4096,
    },
    {
      name: "Grok-4.1",
      provider: "grok",
      apiKey: env.GROK_API_KEY || "",
      endpoint: "https://api.x.ai/v1/chat/completions",
      model: "grok-beta",
      capabilities: ["realtime_search", "research", "english"],
      costTier: 2,
      speedTier: 2,
      maxTokens: 4096,
    },
    {
      name: "Qwen",
      provider: "qwen",
      apiKey: env.SILICONFLOW_API_KEY || "",
      endpoint: "https://api.siliconflow.cn/v1/chat/completions",
      model: "Qwen/Qwen2.5-72B-Instruct",
      capabilities: ["chinese", "code", "general", "cheap"],
      costTier: 1,
      speedTier: 2,
      maxTokens: 4096,
    },
    {
      name: "DeepSeek",
      provider: "deepseek",
      apiKey: env.SILICONFLOW_API_KEY || "",
      endpoint: "https://api.siliconflow.cn/v1/chat/completions",
      model: "deepseek-ai/DeepSeek-V3",
      capabilities: ["reasoning", "code", "math", "cheap"],
      costTier: 1,
      speedTier: 1,
      maxTokens: 4096,
    },
  ].filter((m) => m.apiKey); // åªä¿ç•™æœ‰ API Key çš„æ¨¡å‹
}

/**
 * æ ¹æ®ä»»åŠ¡é€‰æ‹©æœ€ä½³æ¨¡å‹
 */
export function selectModel(preference?: ModelPreference): ModelConfig {
  const registry = getModelRegistry();

  if (registry.length === 0) {
    throw new Error("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·æ£€æŸ¥ API Key é…ç½®");
  }

  // å¦‚æœæŒ‡å®šäº† providerï¼Œç›´æ¥é€‰æ‹©
  if (preference?.provider) {
    const model = registry.find((m) => m.provider === preference.provider);
    if (model) return model;
  }

  // å¦‚æœæŒ‡å®šäº† capabilitiesï¼ŒåŒ¹é…æœ€ä½³
  if (preference?.capabilities && preference.capabilities.length > 0) {
    const scored = registry.map((model) => {
      const matchCount = preference.capabilities!.filter((cap) =>
        model.capabilities.includes(cap)
      ).length;
      return { model, score: matchCount };
    });

    scored.sort((a, b) => b.score - a.score);
    if (scored[0].score > 0) {
      return scored[0].model;
    }
  }

  // é»˜è®¤é€‰æ‹©æˆæœ¬æœ€ä½çš„
  return registry.sort((a, b) => a.costTier - b.costTier)[0];
}

/**
 * è°ƒç”¨ LLM
 */
export async function invokeModel(
  model: ModelConfig,
  messages: ChatMessage[],
  options?: { maxTokens?: number; temperature?: number }
): Promise<LLMResponse> {
  const response = await fetch(model.endpoint, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${model.apiKey}`,
    },
    body: JSON.stringify({
      model: model.model,
      messages,
      max_tokens: options?.maxTokens || model.maxTokens,
      temperature: options?.temperature ?? 0.7,
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`${model.name} API error: ${response.status} - ${errorText}`);
  }

  const data = await response.json();

  return {
    content: data.choices?.[0]?.message?.content || null,
    model: model.name,
    usage: data.usage,
  };
}

/**
 * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
 */
export function getAvailableModels(): string[] {
  return getModelRegistry().map((m) => m.name);
}

/**
 * è·å–æŒ‡å®š provider çš„æ¨¡å‹é…ç½®
 */
export function getModel(provider: ModelProvider): ModelConfig | null {
  return getModelRegistry().find((m) => m.provider === provider) || null;
}
```

**Step 2: éªŒè¯**
```bash
pnpm check
```

---

### CDX-004: é›†æˆå¤šæ¨¡å‹åˆ° llm.ts [Phase 3]

**ç›®æ ‡**: è®©ç°æœ‰çš„ `invokeLLM` ä½¿ç”¨å¤šæ¨¡å‹è·¯ç”±

**Step 1: ä¿®æ”¹ server/_core/llm.ts**

```typescript
import {
  selectModel,
  invokeModel,
  type ModelPreference,
  type ChatMessage,
} from "./model-router";

export interface LLMOptions {
  messages: ChatMessage[];
  maxTokens?: number;
  temperature?: number;
  useThinking?: boolean;
  preferredModel?: ModelPreference;
}

export async function invokeLLM(options: LLMOptions): Promise<{
  choices: Array<{
    message: {
      content: string | null;
    };
  }>;
}> {
  const model = selectModel(options.preferredModel);

  console.log(`[LLM] ä½¿ç”¨æ¨¡å‹: ${model.name}`);

  const response = await invokeModel(model, options.messages, {
    maxTokens: options.maxTokens,
    temperature: options.temperature,
  });

  // è½¬æ¢ä¸ºç°æœ‰æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
  return {
    choices: [
      {
        message: {
          content: response.content,
        },
      },
    ],
  };
}
```

**Step 2: éªŒè¯**
```bash
pnpm check
```

---

## âœ… å®Œæˆæ£€æŸ¥æ¸…å•

```
[ ] CDX-001: useAIStream Hook
    [ ] åˆ›å»º useAIStream.ts
    [ ] å®ç° SSE è¿æ¥é€»è¾‘
    [ ] æ›´æ–° api/index.ts å¯¼å‡º
    [ ] pnpm check é€šè¿‡

[ ] CDX-002: åç«¯ SSE ç«¯ç‚¹
    [ ] æ·»åŠ æµå¼ç«¯ç‚¹åˆ° ai.ts æˆ– Express
    [ ] è¿æ¥ SmartAgent.stream()
    [ ] pnpm check é€šè¿‡

[ ] CDX-003: model-router.ts
    [ ] åˆ›å»ºæ¨¡å‹æ³¨å†Œè¡¨
    [ ] å®ç° selectModel()
    [ ] å®ç° invokeModel()
    [ ] pnpm check é€šè¿‡

[ ] CDX-004: llm.ts é›†æˆ
    [ ] å¯¼å…¥ model-router
    [ ] ä¿®æ”¹ invokeLLM ä½¿ç”¨å¤šæ¨¡å‹
    [ ] pnpm check é€šè¿‡
```

---

## ğŸ›‘ é˜»å¡å¤„ç†

å¦‚æœé‡åˆ°ä»¥ä¸‹æƒ…å†µï¼Œ**ç«‹å³åœä¸‹å¹¶è®°å½•**:

1. **tRPC subscription ä¸æ”¯æŒ**: ä½¿ç”¨åŸç”Ÿ Express SSE
2. **ç¯å¢ƒå˜é‡ç¼ºå¤±**: æ£€æŸ¥ `.env` æ–‡ä»¶
3. **ç±»å‹ä¸åŒ¹é…**: æŸ¥çœ‹ `server/_core/agent/types.ts`

è®°å½•æ ¼å¼:
```
### ğŸ”´ é˜»å¡: [ä»»åŠ¡ID]

**æ—¶é—´**: YYYY-MM-DD HH:MM
**é—®é¢˜æè¿°**: ...
**å°è¯•çš„è§£å†³æ–¹æ¡ˆ**: ...
**éœ€è¦çš„å¸®åŠ©**: ...
```

---

## ğŸ“¤ å®Œæˆå

1. ç¡®ä¿æ‰€æœ‰ä»»åŠ¡ `pnpm check` é€šè¿‡
2. åœ¨ README.md æ›´æ–°ä»»åŠ¡çŠ¶æ€
3. æäº¤ä»£ç :
```bash
git add -A
git commit -m "feat(ai): Codex å®Œæˆæµå¼å“åº”å’Œå¤šæ¨¡å‹è·¯ç”±"
```

---

**ä»»åŠ¡ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¶é—´**: 2026-01-20 21:36
