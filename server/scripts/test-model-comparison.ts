/**
 * æµ‹è¯•ä¸åŒ AI æ¨¡å‹çš„å›ç­”è´¨é‡å¯¹æ¯”
 * 1. GLM-4.7 (å½“å‰é»˜è®¤)
 * 2. Grok (xAI)
 * 3. æ··åˆæ¨¡å¼ (GLM è°ƒåº¦ + Grok åˆ†æ)
 */

import { ENV } from "../_core/env";

interface TestResult {
    model: string;
    query: string;
    response: string;
    duration: number;
    tokens?: number;
}

// ç›´æ¥è°ƒç”¨ GLM API
async function callGLM(query: string, systemPrompt: string): Promise<TestResult> {
    const startTime = Date.now();

    const response = await fetch(`${ENV.glmApiUrl}/chat/completions`, {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${ENV.glmApiKey}`,
        },
        body: JSON.stringify({
            model: ENV.glmModel || "glm-4.7",
            messages: [
                { role: "system", content: systemPrompt },
                { role: "user", content: query },
            ],
            temperature: 0.7,
            max_tokens: 4000,
        }),
    });

    const data = await response.json();
    const content = data.choices?.[0]?.message?.content || "";

    return {
        model: "GLM-4.7",
        query,
        response: content,
        duration: Date.now() - startTime,
        tokens: data.usage?.total_tokens,
    };
}

// ç›´æ¥è°ƒç”¨ Grok API (xAI)
async function callGrok(query: string, systemPrompt: string): Promise<TestResult> {
    const startTime = Date.now();

    if (!ENV.grokApiKey) {
        return {
            model: "Grok",
            query,
            response: "âŒ GROK_API_KEY æœªé…ç½®",
            duration: 0,
        };
    }

    const response = await fetch(`${ENV.grokApiUrl}/chat/completions`, {
        method: "POST",
        headers: {
            "Content-Type": "application/json",
            Authorization: `Bearer ${ENV.grokApiKey}`,
        },
        body: JSON.stringify({
            model: ENV.grokModel || "grok-4-1-fast-reasoning",
            messages: [
                { role: "system", content: systemPrompt },
                { role: "user", content: query },
            ],
            temperature: 0.7,
            max_tokens: 4000,
        }),
    });

    if (!response.ok) {
        const error = await response.text();
        return {
            model: "Grok",
            query,
            response: `âŒ Grok API Error: ${response.status} - ${error}`,
            duration: Date.now() - startTime,
        };
    }

    const data = await response.json();
    const content = data.choices?.[0]?.message?.content || "";

    return {
        model: "Grok",
        query,
        response: content,
        duration: Date.now() - startTime,
        tokens: data.usage?.total_tokens,
    };
}

// æŠ€æœ¯åˆ†æç³»ç»Ÿæç¤ºè¯
const TECH_ANALYSIS_PROMPT = `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Aè‚¡æŠ€æœ¯åˆ†æå¸ˆã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ¡†æ¶è¿›è¡Œåˆ†æï¼š

## åˆ†ææ¡†æ¶

### 1. æœ€æ–°è¡Œæƒ…å›é¡¾
- æ”¶ç›˜ä»·ã€æ¶¨è·Œå¹…ã€æˆäº¤é‡ã€æ¢æ‰‹ç‡ã€é‡æ¯”

### 2. å‡çº¿ç³»ç»Ÿ
- MA5/MA10/MA20/MA50/MA100/MA200 çš„å…·ä½“æ•°å€¼å’Œå¤šç©ºä¿¡å·
- é‡‘å‰/æ­»å‰ä¿¡å·

### 3. MACDæŒ‡æ ‡
- MACDå€¼ã€DIFFã€DEAã€çº¢ç»¿æŸ±çŠ¶æ€
- é‡‘å‰/æ­»å‰ä¿¡å·

### 4. KDJæŒ‡æ ‡
- K/D/Jå€¼ï¼Œè¶…ä¹°è¶…å–çŠ¶æ€

### 5. RSIæŒ‡æ ‡
- RSI(14)å€¼ï¼Œå¼ºå¼±åˆ¤æ–­

### 6. å¸ƒæ—å¸¦
- ä¸Šè½¨/ä¸­è½¨/ä¸‹è½¨ä½ç½®

### 7. æ”¯æ’‘ä½ä¸é˜»åŠ›ä½
- S1/S2/S3æ”¯æ’‘ä½
- R1/R2/R3é˜»åŠ›ä½

### 8. ç»¼åˆç»“è®º
- çŸ­æœŸã€ä¸­æœŸèµ°åŠ¿åˆ¤æ–­
- å…·ä½“æ“ä½œå»ºè®®

è¯·ç»™å‡ºè¯¦ç»†çš„æ•°å€¼å’Œä¸“ä¸šåˆ†æã€‚`;

async function runTests() {
    console.log("ğŸ§ª AI æ¨¡å‹å›ç­”è´¨é‡å¯¹æ¯”æµ‹è¯•\n");
    console.log("=".repeat(70));

    const testQuery = "æŒ‰ç…§æŠ€æœ¯åˆ†ææ¡†æ¶åˆ†æä¸€ä¸‹èˆªå¤©ç”µå­(600879)";

    // æµ‹è¯• 1: GLM
    console.log("\nğŸ“Š æµ‹è¯• 1: GLM-4.7");
    console.log("-".repeat(70));
    try {
        const glmResult = await callGLM(testQuery, TECH_ANALYSIS_PROMPT);
        console.log(`â±ï¸ è€—æ—¶: ${(glmResult.duration / 1000).toFixed(2)}s`);
        console.log(`ğŸ“ é•¿åº¦: ${glmResult.response.length} å­—ç¬¦`);
        console.log(`ğŸ”¢ Tokens: ${glmResult.tokens || "N/A"}`);
        console.log("\nğŸ“„ å›ç­”:");
        console.log(glmResult.response.slice(0, 2000));
        if (glmResult.response.length > 2000) {
            console.log(`\n... (çœç•¥ ${glmResult.response.length - 2000} å­—ç¬¦)`);
        }
    } catch (error: any) {
        console.log(`âŒ GLM æµ‹è¯•å¤±è´¥: ${error.message}`);
    }

    console.log("\n" + "=".repeat(70));

    // æµ‹è¯• 2: Grok
    console.log("\nğŸ“Š æµ‹è¯• 2: Grok (xAI)");
    console.log("-".repeat(70));
    try {
        const grokResult = await callGrok(testQuery, TECH_ANALYSIS_PROMPT);
        console.log(`â±ï¸ è€—æ—¶: ${(grokResult.duration / 1000).toFixed(2)}s`);
        console.log(`ğŸ“ é•¿åº¦: ${grokResult.response.length} å­—ç¬¦`);
        console.log(`ğŸ”¢ Tokens: ${grokResult.tokens || "N/A"}`);
        console.log("\nğŸ“„ å›ç­”:");
        console.log(grokResult.response.slice(0, 2000));
        if (grokResult.response.length > 2000) {
            console.log(`\n... (çœç•¥ ${grokResult.response.length - 2000} å­—ç¬¦)`);
        }
    } catch (error: any) {
        console.log(`âŒ Grok æµ‹è¯•å¤±è´¥: ${error.message}`);
    }

    console.log("\n" + "=".repeat(70));
    console.log("\nâœ… æµ‹è¯•å®Œæˆ");
}

runTests().catch(console.error);
